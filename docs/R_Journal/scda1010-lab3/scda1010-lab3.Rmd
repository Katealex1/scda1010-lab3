---
title: Ranking Applications for Nursery Schools - Relabelling Dataset using Clustering

author: 
  - name          : "Viviane Adohouannon"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=21444"  
  - name          : "Kate Alexander"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=21524"    
  - name          : "Diana Azbel"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=20687"  
  - name          : "Igor Baranov"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/profile.php?id=21219"
abstract: >
  The specific problem under consideration is to rank selection of applicants for nursery schools in Ljubljana, Slovenia in the 1980's. Nursery Database was derived from a hierarchical decision model originally developed to rank applications. A classification model has been used to developed a reliable recomendation algorithm to predict if a specific applicant is a suitable candidate to be addmitted into a nursery school.
output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
---

# Introduction
In our 

Early schooling such as Nursery school education matters most as it impacts children's long-term development and academic progress. While all children benefit from a high-quality nursery school, family structure, social and financial standing, and proximity to schools may affect the enrollment process. 

Nursery dataset \citep{noauthor_uci_nodate} was derived from a hierarchical decision model originally developed to rank applications for nursery schools. It was used during several years in 1980's when there was excessive enrollment to these schools in Ljubljana, Slovenia, and the rejected applications frequently needed an objective explanation.

## Background

In our previous article \citep{scda1010_lab1} we explored the Nursery dataset and using a classification model to developed our algorithm to predict applicants suitabilty of an admittance within the nursery school system, the evaluation of the model was developed using Random Forest algorithm. Even though the original dataset was clear and did not have missing values, it was unballanced. To overcome this a method called Random Over-Sampling was appplied, which increased the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample. The final model achieved almost 99% accuracy of predictions with overall balanced precision more that 97% accross all the predicted "class" values.

Ljubljana is the capital and largest city of Slovenia. The city with an area of 163.8 square kilometers is situated in the Ljubljana Basin in Central Slovenia, between the Alps and the Karst. Ljubljana is located some 320 kilometers south of Munich, 477 kilometers  east of Zurich. In 1981 the population of the city rose to 224,817 inhabitants with approximately 91% of the population speaking Slovene as their primary native language. The second most-spoken language is Bosnian, with Serbo-Croatian being the third most-spoken language \citep{noauthor_ljubljana_2018}.

During this time according to \citep{olave1989application} "new housing developments populated by young families, the demand for childrens admission in nursery schools outstrips supply, notwithstanding the fast growth rate of new schools." In this research, conducted in 1989, an application of expert systems for admission procedures in public
school systems was presented. The specific problem under consideration was selection of applicants for public nursery schools. The selection was supported by an expert system which evaluates, classifies and ranks applications.
The main emphasis was on explanation of the underlying knowledge and solutions, suggested by the system. The system has been developed using DECMAK, an expert system shell for multi-attribute decision making.

In continuation of the search for the practical solution of the problem, second most relevant research was conducted in 1997 \citep{zupan1997machine}. It presented another machine learning method that, given a set of training examples, induced
a definition of the target concept in terms of a hierarchy of intermediate concepts and their definitions. This effectively decomposed the problem into smaller, less complex problems. The method was inspired by the Boolean function decomposition approach to the design of digital circuits. To cope with high time complexity of inding an optimal decomposition,
the authors proposed a suboptimal heuristic algorithm.

It worth to notice that in both cases the researches concentrated on designing and theoretical evaluation of mentioned algorithms without proposing a practical solution that could be deployed in the municipalities and put in use. The reason of this was a relatively high  at the time price of computers that could make predictictions with the models proposed.

## Objective
The objective of this article is to provide a reliable and feasible recommendation algorithm to predict if a specific applicant is a suitable candidate to be admitted into a nursery school or if the applicant should stay with their parents. The results of this recommendation may affect the childs engagement within the school, parents involvement in school activities and overall satisfaction of the applicants long-term academic progress.
  
## Plan
To solve the objective a group of four students calling themselves The First Group (T.F.G) from York University School of Continuing Studies, have come together to create an algorithm using the Nursery dataset. Since the target of this problem is a categorical value representing recommendation if a child is suitable for the admittance to a nursery school, a  supervised classification algorithm was chosen \citep{witten_data_2011}.

The main tool used in developing the recomendation algorithm is R \citep{R}. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.

# Data understanding
The dataset \citep{noauthor_uci_nodate} has 8 attributes and 12960 instances. Creators of the dataset suggested hierarchical model ranks nursery-school applications according to the following concept structure: 

```
NURSERY Evaluation of applications for nursery schools 
. EMPLOY Employment of parents and child's nursery 
. . parents Parents' occupation 
. . has_nurs Child's nursery 
. STRUCT_FINAN Family structure and financial standings 
. . STRUCTURE Family structure 
. . . form Form of the family 
. . . children Number of children 
. . housing Housing conditions 
. . finance Financial standing of the family 
. SOC_HEALTH Social and health picture of the family 
. . social Social conditions 
. . health Health conditions 
```

Nursery Database contains examples with the structural information removed, i.e., directly relates NURSERY to the eight input attributes: parents, has_nurs, form, children, housing, finance, social, health. Data set attribes presented in the following form:

```
parents: usual, pretentious, great_pret 
has_nurs: proper, less_proper, improper, critical, very_crit 
form: complete, completed, incomplete, foster 
children: 1, 2, 3, more 
housing: convenient, less_conv, critical 
finance: convenient, inconv 
social: non-prob, slightly_prob, problematic 
health: recommended, priority, not_recom
```

Target attribute called "class" is a categorical variable having several values that were not revealed in original dataset description and had to be extracted from the data.

\newpage

# Data Preparation
To perform the analysis, certain R libraries were used. The code below was used to load and initialize the libraries. The first line invoking seed function was applied to enforce the repeatability of the calculation results.

```{r message=FALSE, warning=FALSE}
set.seed(77)
library(readr)
library(dplyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(Amelia)
library(rattle)
library(RColorBrewer)
library(caret)
```

The dataset was loaded directly from the dataset site \citep{noauthor_uci_nodate} using the R statement below. Note that column names were assigned as the online data did not have the header. To pretty-print the head of the dataset xtable \citep{R-xtable} library was used to generate Table \ref{table:dhead10}.

```{r message=FALSE, warning=FALSE}
nursery_data <- read.csv(
  "http://archive.ics.uci.edu/ml/machine-learning-databases/nursery/nursery.data", 
  header = FALSE, 
  col.names = 
    c("parents","has_nurs","form","children","housing","finance","social","health","class"))
```


```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# A backup method of loading the dataset in case the site is down
library(readr)
nursery_data <- read.csv(file = "../../../data/nursery_data.csv", 
  colClasses=c("NULL",NA,NA,NA,NA,NA,NA,NA,NA,NA))
```

Thise are the dimensions of our dataset:

```{r message=FALSE, warning=FALSE}
dim(nursery_data)
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
library(xtable)
options(xtable.floating = TRUE)
options(xtable.timestamp = "")
options(xtable.comment = FALSE)

dh.rescale <- xtable(head(nursery_data, n=10), 
  caption = "\\tt Nursery Data Dataset (head)", label = "table:dhead10")

print(dh.rescale, scalebox=.75)
```

Summary of Nursery Data set is extracted by the R summary function, the results are presented below.
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
summary(nursery_data)
```


## Reordering Dataset Factor Leveks

Very often, especially when plotting data, we need to reorder the levels of a factor because the default order is alphabetical. A direct way of reordering, using standard syntax is as follows. Current levels, need to be corrected to correspont to the dataset description.

```{r echo=FALSE, message=FALSE, warning=FALSE}
print (levels(nursery_data$parents))
print (levels(nursery_data$has_nurs))
print (levels(nursery_data$form))
print (levels(nursery_data$children))
print (levels(nursery_data$housing))
print (levels(nursery_data$finance))
print (levels(nursery_data$social))
print (levels(nursery_data$health))
print (levels(nursery_data$class))
```

Correction:

```{r}
nursery_data$parents <- factor(nursery_data$parents,levels(nursery_data$parents)[c(3,2,1)])
nursery_data$has_nurs <- factor(nursery_data$has_nurs,levels(nursery_data$has_nurs)[c(4,3,2,1,5)])
nursery_data$form <- factor(nursery_data$form,levels(nursery_data$form)[c(1,2,4,3)])
nursery_data$children <- factor(nursery_data$children,levels(nursery_data$children)[c(1,2,3,4)])
nursery_data$housing <- factor(nursery_data$housing,levels(nursery_data$housing)[c(1,3,2)])
nursery_data$finance <- factor(nursery_data$finance,levels(nursery_data$finance)[c(1,2)])
nursery_data$social <- factor(nursery_data$social,levels(nursery_data$social)[c(1,3,2)])
nursery_data$health <- factor(nursery_data$health,levels(nursery_data$health)[c(1,3,2)])
nursery_data$class <- factor(nursery_data$class,levels(nursery_data$class)[c(1,3,5,2,4)])
```

Corrected levels, now correspond to the dataset description:

```{r echo=FALSE, message=FALSE, warning=FALSE}
print (levels(nursery_data$parents))
print (levels(nursery_data$has_nurs))
print (levels(nursery_data$form))
print (levels(nursery_data$children))
print (levels(nursery_data$housing))
print (levels(nursery_data$finance))
print (levels(nursery_data$social))
print (levels(nursery_data$health))
print (levels(nursery_data$class))
```

## Convert to numbers in one step

Return the matrix obtained by converting all the variables in a data frame to numeric mode and then binding them together as the columns of a matrix. Factors and ordered factors are replaced by their internal codes. Logical and factor columns are converted to integers. 

```{r}
data <- data.matrix(nursery_data)
head(data)
```

## Preparing scaled data
```{r}
#index <- sample(1:nrow(data),round(0.75*nrow(data)))
#index <- createDataPartition(y= data$QLT, p=0.5, list = FALSE)
maxs <- apply(data, 2, max) 
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
#train_ <- scaled[index,]
#test_ <- scaled[-index,]
```

# The problem

## Distribution of target value in the dataset
The target value class of the wine quality is not equally distributed. The Figure \ref{fig:hist_class} demonstrates the distribution. As we can see, dataset covers mostly medium-quality wines with QLT between 5 and 7 well, low and high  quality wines represented poorly.
```{r}
prop.table(table(nursery_data$class))
```


```{r hist_class, fig.pos = 'h', fig.height=3, fig.width=5, fig.align="center", fig.cap="Distribution the Target 'class' Attribute in the Nursery Dataset"}
ggplot(data = nursery_data, mapping = aes(x = class)) + geom_bar()
```

\newpage

# Clustering

A fundamental question is how to determine the value of the parameter k. 
If we looks at the percentage of variance explained as a function of the number of clusters: 
One should choose a number of clusters so that adding another cluster doesn't give much better 
modeling of the data. More precisely, if one plots the percentage of variance explained by the 
clusters against the number of clusters, the first clusters will add much information 
(explain a lot of variance), but at some point the marginal gain will drop, 
giving an angle in the graph. The number of clusters is chosen at this point, hence the 'elbow criterion'.
The diagram presented in Figure \ref{fig:elbow}

```{r elbow,  fig.height=6, fig.width=5.5, message=FALSE, warning=FALSE, fig.cap="Elbow Criterion Diagram"}
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(scaled, nc=15) 
```

## Clustering using K-means method

k-means clustering \citep{noauthor_k-means_2018} is a method of vector quantization, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells \citep{noauthor_nouvelles_1908}.

The problem is computationally difficult (NP-hard), k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes. The algorithm has a loose relationship to the k-nearest neighbor classifier. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.


```{r  fig.height=10, fig.width=10}
set.seed(420)
clusters_num =5
k.means.fit <- kmeans(scaled, clusters_num,iter.max = 1000)
attributes(k.means.fit)
k.means.fit$centers
k.means.fit$size
```


```{r fig.height=9, fig.width=6, fig.cap="2D representation of the Cluster solution"}
library(cluster)
clusplot(scaled, k.means.fit$cluster, main='',
         color=TRUE, shade=FALSE,
         labels=clusters_num, lines=0)
```

# Explain clusters

## Explain by 'class' 

Let's try to explain clusters by the 'class'. Code below builds a matrix whe columns are cluster numbers and rows are target classes.

```{r}
table(nursery_data$class,k.means.fit$cluster)
```

# Hierarchical Clustering

k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation of the algorithm is that it cannot be used with arbitrary distance functions or on non-numerical data.

Hierarchical methods uses a distance matrix as an input for the clustering algorithm. 
The choice of an appropriate metric will influence the shape of the clusters, as some element
may be close to one another according to one distance and farther away according to another.
We use the Euclidean distance as an input for the clustering algorithm 
ward.2D minimum variance criterion minimizes the total within-cluster variance.

```{r}
d <- dist(scaled, method = "manhattan")
H.fit <- hclust(d, method="ward.D2")
```

The clustering output is displayed in a dendrogram \ref{fig:dendr}.

```{r dendr, fig.height=9, fig.hight=8, fig.width=6, message=FALSE, warning=FALSE}
clusters_num = 20
plot(H.fit)
groups <- cutree(H.fit, k=clusters_num)
rect.hclust(H.fit, k=clusters_num, border="red") 
```

The clustering performance can be evaluated with the aid of a confusion matrix as follows. Let's look at the groups that have mixed valued of 'class'. Group 1 contains class 'recommend' and also 'very_recom' and 'priority'. Since our idea is relable rows to 'recommend' to increase it's presense, let's check if there is any justification to do this.


```{r}
table(nursery_data$class,groups)
```

Let's find what are the most significant factors that separate group 1 from also mixed group 5. It looks that group has better financial situation but 22% less favorable family structure and 12% better housing situation, with the rest of the attributes beeing close - less that 1%. We could arbitrary decide that it is justified to downgade group 1 grading all the fows to 'recommend' even thogh most of the rows were previously been labeled higher.

```{r}
dif <- colMeans(scaled[groups == 1,]) - colMeans(scaled[groups == 5,])
dif <- dif[order(abs(dif), decreasing = T)]
print(dif)
```

Group 5 has high amount of 'very_recommend' values in addition to 'priority' and some 'special_priority'. Let's find what are the most significant factors that separate group 5 from also mixed similar group 9. Looking at the most influential attribus defining the difference between those groups, it looks that group 5 has less favorable financial situation, but 50% better housing situation and 30% better formal family structure. Again we could arbitrary decide that it is justified to downgrade group 5 down grading all the records in the group as 'very_recommend' even though most of the rows were previously been labeled higher.

```{r}
dif <- colMeans(scaled[groups == 5,]) - colMeans(scaled[groups == 9,])
dif <- dif[order(abs(dif), decreasing = T)]
print(dif)
```

Lets fix the groups 1 and 5 by relabelling class according to our conclusions:

```{r}
nursery_data[groups == 1,]$class<- 'recommend'
nursery_data[groups == 5,]$class<- 'very_recom'
```

Lets analyze the result visualizing the distribution of class now. Obviosly  the distribution is improved compaired to previous data.  

```{r}
prop.table(table(nursery_data$class))
```

```{r hist2_class, fig.pos = 'h', fig.height=3, fig.width=5, fig.align="center", fig.cap="Distribution the Target 'class' Attribute in the Nursery Dataset"}
ggplot(data = nursery_data, mapping = aes(x = class)) + geom_bar()
```

# Modeling

## Random Forest model prediction and evaluation of the original dataset

The Nursery dataset has been split in such a way that train and test sets would have the same distribution of the 'class' attribute. The reason for this stratification strategy is to focus on the priority of an applicants placement in a nursery school rather than an applicants family or social status. We used 75:25 split ratio. 

```{r}
train.rows<- createDataPartition(y= nursery_data$class, p=0.75, list = FALSE)
train.data<- nursery_data[train.rows,]
prop.table((table(train.data$class)))
```

```{r}
test.data<- nursery_data[-train.rows,]
prop.table((table(test.data$class)))
```

```{r message=FALSE, warning=FALSE}
library(randomForest)
fitRF2 <- randomForest(
  class~parents+has_nurs+form+children+housing+finance+social+health,
  method="anova",
  data=train.data, importance=TRUE, ntree=1000 )
```

Now let's calculate prediction and it's evaluation using the corrected dataset. As we expected, now all but one of the "very_recom" values of the target "class" attribute were predicted correctly which led to total accuracy of the prediction to 98% with overall balanced precision accross all the predicted "class" values.

```{r}
PredictionRF2 <- predict(fitRF2, test.data)
confMat2 <- table(PredictionRF2,test.data$class)
confMat2
accuracy <- sum(diag(confMat2))/sum(confMat2)
cat(sprintf("\nAccuracy=%f", accuracy))
```


To visualize the results of the predictions, the code below generates a scatter plot of the Predictor vs Test values (Figure \ref{fig:plot_rf_rw}). Also a QQ-Plot for Studentized Residuals is generated and presented in Figure \ref{fig:qq_rf_rw}.

```{r plot_rf_rw, fig.width=5.5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Random Forest Prediction"}
library(ggplot2)
df2 = data.frame(test.data$class, PredictionRF2)
colnames(df2) <- c("Test","Prediction")
ggplot(df2, aes(x = Test, y = Prediction)) +
        geom_jitter(width = 0.2, pch=20, col=rgb(0.1, 0.2, 0.8, 0.3))
```


\newpage
# Conclusion
Through exploring the Nursery dataset and using a classification model to developed our algorithm to predict applicants suitabilty of an admittance within the nursery school system, the evaluation of the model was developed using Random Forest algorithm. Even though the original dataset was clear and did not have missing values, it was unballanced. To overcome this a method called Random Over-Sampling was appplied, which increased the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample. The final model achieved almost 99% accuracy of predictions with overall balanced precision more that 97% accross all the predicted "class" values. The project was a success.

\bibliography{RJreferences}

\newpage
# Note from the Authors
This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/scda1010-lab3/tree/master/docs/R_Journal/scda1010-lab3) with all the necessary artifacts.
